**How to Run**
- download Ollama: https://ollama.com/

**On the Windows Powershell**
- ollama run llama3.2:1b (The least resource consuming model)
- ollama serve (if error stating port already in use, then proceed with next command, as ollama is already running)
- ollama pull llama3 

**On terminal or VSCode**
- Run program for specific pupose
- setup.py to install requieremnts and setup envionments 
- llm.py for testing llm

**Additional PDF Upload**
- If additional PDFs are required for upload, add PDFs to Data folder, before running main.py.

